{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Scikit imports\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting notebook\n",
    "\n",
    "Notebook for overfitting simulation and notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete labels\n",
    "\n",
    "### `make_classification`\n",
    "\n",
    "- clusters of points normally distributed with $\\sigma=1$ about vertices of an `n_informative`-dimensional hypercube with sides of length 2*`class_sep`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "n_informative = 4\n",
    "n_redundant = 2\n",
    "n_total_feats = 20\n",
    "\n",
    "# samples\n",
    "n_train = 200\n",
    "n_test = 200\n",
    "\n",
    "# class parameters\n",
    "n_classes = 2\n",
    "weights = None # defaults to 50/50 split\n",
    "\n",
    "# output params\n",
    "shuffle = True\n",
    "random_state = 0\n",
    "class_sep = 1\n",
    "n_clusters_per_class = 4\n",
    "\n",
    "X, y = make_classification(\n",
    "                            n_samples = n_train + n_test,\n",
    "                            n_features = n_total_feats,\n",
    "                            n_informative = n_informative,\n",
    "                            n_redundant = n_redundant,\n",
    "                            n_classes = n_classes,\n",
    "                            weights = weights,\n",
    "                            shuffle = shuffle,\n",
    "                            random_state = random_state,\n",
    "                            class_sep = class_sep,\n",
    "                            n_clusters_per_class = n_clusters_per_class)\n",
    "\n",
    "end_idx_0 = n_train // 2\n",
    "start_idx_1 = (n_train // 2) + (n_test // 2)\n",
    "end_idx_1 = n_train + (n_test // 2)\n",
    "train_data = np.append(X[:end_idx_0, :], X[start_idx_1:end_idx_1, :], 0)\n",
    "train_data = np.c_[train_data, np.append(y[:end_idx_0], y[start_idx_1:end_idx_1])]\n",
    "\n",
    "test_data = np.append(X[end_idx_0:start_idx_1, :], X[end_idx_1:, :], 0)\n",
    "test_data = np.c_[test_data, np.append(y[end_idx_0:start_idx_1], y[end_idx_1:])]\n",
    "\n",
    "train_X = train_data[:, :-1]\n",
    "train_y = train_data[:, -1]\n",
    "\n",
    "test_X = test_data[:, :-1]\n",
    "test_y = test_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.0\n",
      "96.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_y))\n",
    "print(sum(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[0.62, 0.67, 0.575, 0.565]\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,5)\n",
    "\n",
    "\n",
    "train_acc = []\n",
    "test_proba = []\n",
    "test_acc = []\n",
    "pipelines = []\n",
    "for deg in degrees:\n",
    "    poly_features = PolynomialFeatures(degree=deg, \n",
    "                                       include_bias=False,\n",
    "                                       interaction_only=True)\n",
    "    lr_model = LogisticRegression(solver='lbfgs', \n",
    "                                  #penalty='none', \n",
    "                                  C=10000000,\n",
    "                                  random_state=random_state,\n",
    "                                  n_jobs=-1)\n",
    "    lr_pipeline = Pipeline([(\"polynomial_features\", poly_features),\n",
    "                           (\"logistic_regression\", lr_model)])\n",
    "    \n",
    "    lr_pipeline.fit(train_X.copy(), train_y.copy())\n",
    "    \n",
    "    pipelines.append(lr_pipeline)\n",
    "    \n",
    "    train_acc.append(accuracy_score(train_y, lr_pipeline.predict(train_X)))\n",
    "    test_proba.append(np.mean(lr_pipeline.predict_proba(test_X)[:, 0]))\n",
    "    test_acc.append(accuracy_score(test_y, lr_pipeline.predict(test_X)))\n",
    "    print(deg)\n",
    "    \n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75, 1.0, 1.0]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5034791175509974,\n",
       " 0.5156326748810355,\n",
       " 0.5255363858212497,\n",
       " 0.5431345289257606,\n",
       " 0.5251316294044792,\n",
       " 0.5458332000115349,\n",
       " 0.5396144508000041,\n",
       " 0.522354837583173,\n",
       " 0.5309405091323608]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55, 0.6, 0.545, 0.505, 0.52, 0.51, 0.545, 0.52, 0.485]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.517117387804012"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label_X = np.random.multivariate_normal(np.random.rand((n_total_feats)), \n",
    "                                           cov=np.eye(n_total_feats),\n",
    "                                           size=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proba = []\n",
    "test_pred = []\n",
    "for pipe in pipelines:\n",
    "    pred_prob = pipe.predict_proba(no_label_X)\n",
    "    #print(pred_prob.shape)\n",
    "    #print(pred_prob[:5,:])\n",
    "    test_proba.append(np.mean(pred_prob[:, 1]))\n",
    "    test_pred.append(np.sum(pipe.predict(no_label_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43318725779486056,\n",
       " 0.42784594267590864,\n",
       " 0.5799342239135585,\n",
       " 0.589066750563704,\n",
       " 0.5723469159133173,\n",
       " 0.5436274211233478,\n",
       " 0.50859931179229,\n",
       " 0.5219533978035353,\n",
       " 0.532691651530601]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73.0, 85.0, 114.0, 118.0, 119.0, 113.0, 106.0, 111.0, 112.0]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "210\n",
      "1350\n",
      "6195\n",
      "21699\n",
      "60459\n",
      "137979\n",
      "263949\n",
      "431909\n"
     ]
    }
   ],
   "source": [
    "for pipe in pipelines:\n",
    "    print(pipe['polynomial_features'].n_output_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- generate data from two separate distributions\n",
    "- plots as $p$ increases\n",
    "- train on 50/50 split\n",
    "- plots of underfit, \"fit,\" and overfit predictions as the distribution of out-of-sample data goes from 0 to 1 in terms of labels\n",
    "- use `make_classification`\n",
    "- use pipeline of polynomial features\n",
    "- TODO do the out-of-sample people come from a different data distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
