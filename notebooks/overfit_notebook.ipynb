{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Scikit imports\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting notebook\n",
    "\n",
    "Notebook for overfitting simulation and notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete labels\n",
    "\n",
    "### `make_classification`\n",
    "\n",
    "- clusters of points normally distributed with $\\sigma=1$ about vertices of an `n_informative`-dimensional hypercube with sides of length 2*`class_sep`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "n_informative = 4\n",
    "n_redundant = 2\n",
    "n_total_feats = 20\n",
    "\n",
    "# samples\n",
    "n_train = 100\n",
    "n_test = 200\n",
    "\n",
    "# class parameters\n",
    "n_classes = 2\n",
    "weights = None # defaults to 50/50 split\n",
    "\n",
    "# output params\n",
    "shuffle = True\n",
    "random_state = 0\n",
    "class_sep = 1\n",
    "n_clusters_per_class = 4\n",
    "\n",
    "X, y = make_classification(\n",
    "                            n_samples = n_train + n_test,\n",
    "                            n_features = n_total_feats,\n",
    "                            n_informative = n_informative,\n",
    "                            n_redundant = n_redundant,\n",
    "                            n_classes = n_classes,\n",
    "                            weights = weights,\n",
    "                            shuffle = shuffle,\n",
    "                            random_state = random_state,\n",
    "                            class_sep = class_sep)\n",
    "\n",
    "end_idx_0 = n_train // 2\n",
    "start_idx_1 = (n_train // 2) + (n_test // 2)\n",
    "end_idx_1 = n_train + (n_test // 2)\n",
    "train_data = np.append(X[:end_idx_0, :], X[start_idx_1:end_idx_1, :], 0)\n",
    "train_data = np.c_[train_data, np.append(y[:end_idx_0], y[start_idx_1:end_idx_1])]\n",
    "\n",
    "test_data = np.append(X[end_idx_0:start_idx_1, :], X[end_idx_1:, :], 0)\n",
    "test_data = np.c_[test_data, np.append(y[end_idx_0:start_idx_1], y[end_idx_1:])]\n",
    "\n",
    "train_X = train_data[:, :-1]\n",
    "train_y = train_data[:, -1]\n",
    "\n",
    "test_X = test_data[:, :-1]\n",
    "test_y = test_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.0\n",
      "104.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_y))\n",
    "print(sum(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "[0.71, 0.845, 0.695, 0.635, 0.54, 0.53, 0.49, 0.515]\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,9)\n",
    "\n",
    "\n",
    "train_acc = []\n",
    "test_proba = []\n",
    "test_acc = []\n",
    "pipelines = []\n",
    "for deg in degrees:\n",
    "    poly_features = PolynomialFeatures(degree=deg, \n",
    "                                       include_bias=False,\n",
    "                                       interaction_only=True)\n",
    "    lr_model = LogisticRegression(solver='lbfgs', \n",
    "                                  penalty='none', \n",
    "                                  #C=1000,\n",
    "                                  random_state=random_state,\n",
    "                                  n_jobs=-1)\n",
    "    lr_pipeline = Pipeline([(\"polynomial_features\", poly_features),\n",
    "                           (\"logistic_regression\", lr_model)])\n",
    "    \n",
    "    lr_pipeline.fit(train_X.copy(), train_y.copy())\n",
    "    \n",
    "    pipelines.append(lr_pipeline)\n",
    "    \n",
    "    train_acc.append(accuracy_score(train_y, lr_pipeline.predict(train_X)))\n",
    "    test_proba.append(np.mean(lr_pipeline.predict_proba(test_X)[:, 0]))\n",
    "    test_acc.append(accuracy_score(test_y, lr_pipeline.predict(test_X)))\n",
    "    print(deg)\n",
    "    \n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.84, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4975156411446495,\n",
       " 0.5147213608980707,\n",
       " 0.5203572343000207,\n",
       " 0.5325925971670543,\n",
       " 0.5128486797771346,\n",
       " 0.516220334203848,\n",
       " 0.5327722450695098,\n",
       " 0.5244140952639429]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.59236389408978"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label_X = np.random.multivariate_normal(np.random.rand((n_total_feats)), \n",
    "                                           cov=np.eye(n_total_feats),\n",
    "                                           size=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proba = []\n",
    "test_pred = []\n",
    "for pipe in pipelines:\n",
    "    pred_prob = pipe.predict_proba(no_label_X)\n",
    "    #print(pred_prob.shape)\n",
    "    #print(pred_prob[:5,:])\n",
    "    test_proba.append(np.mean(pred_prob[:, 1]))\n",
    "    test_pred.append(np.sum(pipe.predict(no_label_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8791821831992749,\n",
       " 0.44951614452253735,\n",
       " 0.4969247489110623,\n",
       " 0.48669858358926105,\n",
       " 0.4604947046060801,\n",
       " 0.48182724817254197,\n",
       " 0.47941191581319686,\n",
       " 0.451512202326811]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[186.0, 90.0, 99.0, 95.0, 88.0, 98.0, 94.0, 79.0]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "210\n",
      "1350\n",
      "6195\n",
      "21699\n",
      "60459\n",
      "137979\n",
      "263949\n"
     ]
    }
   ],
   "source": [
    "for pipe in pipelines:\n",
    "    print(pipe['polynomial_features'].n_output_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- generate data from two separate distributions\n",
    "- plots as $p$ increases\n",
    "- train on 50/50 split\n",
    "- plots of underfit, \"fit,\" and overfit predictions as the distribution of out-of-sample data goes from 0 to 1 in terms of labels\n",
    "- use `make_classification`\n",
    "- use pipeline of polynomial features\n",
    "- TODO do the out-of-sample people come from a different data distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
